{"cells":[{"cell_type":"markdown","metadata":{"id":"LN0nZwyMGadB"},"source":["# Pong with Advantage Actor Critic\n","\n","## Step 1: Import the libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7233,"status":"ok","timestamp":1676631919977,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"YDqfP_5cK2H1"},"outputs":[],"source":["!pip install requirements.txt"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1676631930517,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"Jp3rMN-2MwkV","outputId":"ccc3bc08-6099-4a1d-8aa5-8b4306dd0e2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["AutoROM will download the Atari 2600 ROMs.\n","They will be installed to:\n","\t/usr/local/lib/python3.8/dist-packages/AutoROM/roms\n","\n","Existing ROMs will be overwritten.\n","robotank.bin\n","time_pilot.bin\n","space_invaders.bin\n","et.bin\n","ms_pacman.bin\n","video_pinball.bin\n","laser_gates.bin\n","phoenix.bin\n","flag_capture.bin\n","name_this_game.bin\n","star_gunner.bin\n","private_eye.bin\n","freeway.bin\n","human_cannonball.bin\n","battle_zone.bin\n","pitfall.bin\n","beam_rider.bin\n","kaboom.bin\n","haunted_house.bin\n","road_runner.bin\n","mr_do.bin\n","riverraid.bin\n","warlords.bin\n","enduro.bin\n","chopper_command.bin\n","miniature_golf.bin\n","air_raid.bin\n","video_chess.bin\n","alien.bin\n","word_zapper.bin\n","turmoil.bin\n","breakout.bin\n","fishing_derby.bin\n","crazy_climber.bin\n","joust.bin\n","journey_escape.bin\n","kung_fu_master.bin\n","space_war.bin\n","entombed.bin\n","koolaid.bin\n","demon_attack.bin\n","gopher.bin\n","__pycache__\n","mario_bros.bin\n","lost_luggage.bin\n","asterix.bin\n","kangaroo.bin\n","tic_tac_toe_3d.bin\n","video_cube.bin\n","solaris.bin\n","ice_hockey.bin\n","tetris.bin\n","assault.bin\n","galaxian.bin\n","wizard_of_wor.bin\n","backgammon.bin\n","amidar.bin\n","klax.bin\n","krull.bin\n","keystone_kapers.bin\n","up_n_down.bin\n","basic_math.bin\n","trondead.bin\n","berzerk.bin\n","blackjack.bin\n","zaxxon.bin\n","combat.bin\n","earthworld.bin\n","tutankham.bin\n","casino.bin\n","jamesbond.bin\n","tennis.bin\n","frostbite.bin\n","centipede.bin\n","montezuma_revenge.bin\n","pooyan.bin\n","skiing.bin\n","atlantis.bin\n","crossbow.bin\n","yars_revenge.bin\n","carnival.bin\n","__init__.py\n","pong.bin\n","darkchambers.bin\n","double_dunk.bin\n","donkey_kong.bin\n","maze_craze.bin\n","pitfall2.bin\n","elevator_action.bin\n","asteroids.bin\n","boxing.bin\n","bowling.bin\n","video_checkers.bin\n","hangman.bin\n","qbert.bin\n","frogger.bin\n","othello.bin\n","hero.bin\n","surround.bin\n","adventure.bin\n","king_kong.bin\n","bank_heist.bin\n","gravitar.bin\n","atlantis2.bin\n","venture.bin\n","superman.bin\n","sir_lancelot.bin\n","pacman.bin\n","seaquest.bin\n","defender.bin\n"]}],"source":["!AutoROM -y"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":320,"status":"ok","timestamp":1676631930826,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"1NzUA2e6KeMh","outputId":"07679f23-489d-4ccd-d731-91aa9e152d2f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/gymnasium/envs/registration.py:521: UserWarning: \u001b[33mWARN: Overriding environment GymV26Environment-v0 already in registry.\u001b[0m\n","  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n","/usr/local/lib/python3.8/dist-packages/gymnasium/envs/registration.py:521: UserWarning: \u001b[33mWARN: Overriding environment GymV22Environment-v0 already in registry.\u001b[0m\n","  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"]}],"source":["import gymnasium as gym\n","import torch\n","import numpy as np\n","from collections import deque\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","from random import sample\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9573,"status":"ok","timestamp":1676631940395,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"NCc627GrK_as","outputId":"75d9fc85-4d3e-40cf-ab6a-a1b54662fc93"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1676631940396,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"sGyeQvKELPTT","outputId":"6718d602-2aa5-4bb6-da8b-f3c25fb67afc"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/M2DS/Reinforcement_learning/notebooks\n"]}],"source":["#%cd '/content/drive/MyDrive/Reinforcement_learning/notebooks'\n","%cd 'drive/My Drive/Colab Notebooks/M2DS/Reinforcement_learning/notebooks'"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1676631940396,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"OxQZW66kKeMi"},"outputs":[],"source":["import sys\n","sys.path.append('../../')\n","from a2c_agent import A2CAgent\n","from actor_critic_cnn import ActorCnn, CriticCnn"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1676631940397,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"3G75uw96KeMj"},"outputs":[],"source":["from gymnasium.wrappers import FrameStack"]},{"cell_type":"markdown","metadata":{"id":"tfo8jleHGadK"},"source":["## Step 1: Create our environment\n","\n","Initialize the environment in the code cell below.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":858,"status":"ok","timestamp":1676631941248,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"6c3LiIjEKeMk","outputId":"37a47e9f-04d3-4ce6-a49e-a8c2093f39a8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/gymnasium/utils/passive_env_checker.py:35: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (210, 160)\u001b[0m\n","  logger.warn(\n"]}],"source":["env = gym.make(\"Pong-v4\", mode=1, obs_type=\"grayscale\")\n","env = gym.wrappers.ResizeObservation(env, (84, 84))\n","env = FrameStack(env, 4)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1676631941249,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"Viuwg9HNKeMk","outputId":"6536c714-420c-4c68-de71-02f287fb9269"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device:  cuda\n"]}],"source":["# if gpu is to be used\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device: \", device)"]},{"cell_type":"markdown","metadata":{"id":"EXFclebIKeMo"},"source":["## Step 2: Creating our Agent"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1676631941249,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"1vZHoxwfKeMp"},"outputs":[],"source":["INPUT_SHAPE = (4, 84, 84)\n","ACTION_SIZE = env.action_space.n\n","SEED = 0\n","GAMMA = 0.99           # discount factor\n","ALPHA= 0.0001          # Actor learning rate\n","BETA = 0.0005          # Critic learning rate\n","UPDATE_EVERY = 5     # how often to update the network \n","\n","agent = A2CAgent(INPUT_SHAPE, ACTION_SIZE, SEED, device, GAMMA, ALPHA, BETA, UPDATE_EVERY, ActorCnn, CriticCnn)"]},{"cell_type":"markdown","metadata":{"id":"xUG9-NaqQEHA"},"source":["## Step 3: Creating our HumanRewarder"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1676631941250,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"hHRdYIvsQINV"},"outputs":[],"source":["from reward_function import HumanFeedBackRewardFunction"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1676631941250,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"QRDPnHbVQc5W"},"outputs":[],"source":["human_rewarder = HumanFeedBackRewardFunction(device)"]},{"cell_type":"markdown","metadata":{"id":"PmTBIOqlixPd"},"source":["## Step 4: Creating human choice"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1676631941251,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"pSzTlCUdi1GS"},"outputs":[],"source":["def getHumanChoice(obs_1, obs_2):\n","    obs_1 = (np.array(obs_1) * 255).astype(np.uint8)\n","    obs_2 = (np.array(obs_2) * 255).astype(np.uint8)\n","\n","    # obs_1 = obs_1.reshape((obs_1.shape[0]*obs_1.shape[1], obs_1.shape[2], obs_1.shape[3]))\n","    # obs_2 = obs_2.reshape((obs_2.shape[0]*obs_2.shape[1], obs_2.shape[2], obs_2.shape[3]))\n","\n","    imageio.mimsave(\n","        \"./example1.gif\", obs_1, fps=15  # output gif  # array of input frames\n","    )\n","\n","    imageio.mimsave(\"./example2.gif\", obs_2, fps=15)\n","\n","    clear_output(True)\n","\n","    print(\"Choose between 2 scenarios:\")\n","    print(\n","        \"0: They are equally good, 1: The 1st is better, 2: The 2nd is better, 3: None are good\"\n","    )\n","\n","    img1 = open(\"example1.gif\", \"rb\").read()\n","    img2 = open(\"example2.gif\", \"rb\").read()\n","\n","    wi1 = widgets.Image(value=img1, format=\"png\", width=300, height=400)\n","    wi2 = widgets.Image(value=img2, format=\"png\", width=300, height=400)\n","\n","    sidebyside = widgets.HBox([wi1, wi2])\n","\n","    display(sidebyside)\n","\n","    choice = input(\"Press 0 or 1 or 2 or 3:\")\n","\n","    if choice == 0:\n","        human_choice = [0.5, 0.5]\n","    elif choice == 1:\n","        human_choice = [1, 0]\n","    elif choice == 2:\n","        human_choice = [0, 1]\n","    else:\n","        human_choice = [0, 0]\n","\n","    return human_choice"]},{"cell_type":"markdown","metadata":{"id":"hO6yMyvgka5g"},"source":["## Step 5: Initialize D space"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1676631941252,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"kIgXZAl7kadm"},"outputs":[],"source":["def initialize_D_space(real_human_check=False):\n","    print(\"initialize_D_space: Start\")\n","\n","    D_space = []\n","\n","    while len(D_space) < 500:\n","        obs, _ = env.reset()\n","        time_step = 0\n","        obs_1_list = []\n","        obs_2_list = []\n","        real_reward_1 = 0\n","        real_reward_2 = 0\n","\n","        obs = np.array(obs._frames)\n","        obs = np.ascontiguousarray(obs, dtype=np.float32) / 255\n","\n","        while len(D_space) < 500:\n","            time_step += 1\n","\n","            action, log_prob, entropy = agent.act(obs)\n","            next_obs, reward, terminated, truncated, info = env.step(action)\n","\n","            next_obs = np.array(next_obs._frames)\n","            next_obs = np.ascontiguousarray(next_obs, dtype=np.float32) / 255\n","\n","            obs = next_obs\n","\n","            if time_step <= 25:\n","                obs_1_list.append(\n","                    obs[-1]\n","                )  # We keep only the last frame since 1 step = 1 frame\n","                real_reward_1 += reward\n","            else:\n","                obs_2_list.append(\n","                    obs[-1]\n","                )  # We keep only the last frame since 1 step = 1 frame\n","                real_reward_2 += reward\n","\n","            if time_step >= 50:\n","                if real_human_check:\n","                    human_choice = getHumanChoice(obs_1_list, obs_2_list)\n","                else:\n","                    # Here we fake the behavior of our real human assessor\n","                    if real_reward_1 > real_reward_2:\n","                        human_choice = [1, 0]\n","                    else:\n","                        human_choice = [0, 1]\n","\n","                if human_choice != [0, 0]:\n","                    D_space.append(np.array([obs_1_list, obs_2_list, human_choice]))\n","\n","                obs_1_list = []\n","                obs_2_list = []\n","                time_step = 0\n","\n","    print(\"initialize_D_space: End\")\n","\n","    return D_space"]},{"cell_type":"markdown","metadata":{"id":"Bj_R7hFLopQ9"},"source":["## Step 6: Creating HumanRewarder Train Function"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1676631941252,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"ssJVcNgvoose"},"outputs":[],"source":["# We shall call this function each 10 timesteps\n","def trainHumanRewarder(D_space):\n","    i = 10  # We sample 10 triplets from D\n","\n","    D_sampled = sample(D_space, i)\n","\n","    fake_reward_1_list_all = torch.Tensor([])  # (i, 3) --> i segments and 3 rewarder\n","    fake_reward_2_list_all = torch.Tensor([])\n","\n","    human_choice_list = []\n","\n","    for triplet in D_sampled:\n","        fake_reward_1_list = []  # (1, 3) --> 1 segment duos and 3 rewarder\n","        fake_reward_2_list = []\n","\n","        human_choice_list.append(triplet[2])\n","\n","        for n in range(0, len(triplet[0]) - 1, 4):\n","            obs_1_i = np.array(triplet[0][n : n + 4])\n","            obs_2_i = np.array(triplet[1][n : n + 4])\n","\n","            fake_reward_1_list.append(human_rewarder(obs_1_i, True))  # We append (1,3)\n","            fake_reward_2_list.append(human_rewarder(obs_2_i, True))\n","\n","        # fake_reward_1_list = torch.sum(torch.Tensor(fake_reward_1_list), axis = 0) #From (6,3) to (1, 3)\n","        # fake_reward_2_list = torch.sum(torch.Tensor(fake_reward_2_list), axis = 0)\n","        fake_reward_1_list = torch.sum(\n","            fake_reward_1_list, axis=0\n","        )  # From (6,3) to (1, 3)\n","        fake_reward_2_list = torch.sum(fake_reward_2_list, axis=0)\n","\n","        # fake_reward_1_list_all.append(fake_reward_1_list)\n","        if len(fake_reward_1_list_all) == 0:\n","            fake_reward_1_list_all = fake_reward_1_list.unsqueeze(0)\n","            fake_reward_2_list_all = fake_reward_2_list.unsqueeze(0)\n","        else:\n","            fake_reward_1_list_all = torch.cat(\n","                (fake_reward_1_list_all, fake_reward_1_list.unsqueeze(0)), 0\n","            )\n","            fake_reward_2_list_all = torch.cat(\n","                (fake_reward_2_list_all, fake_reward_2_list.unsqueeze(0)), 0\n","            )\n","\n","        # fake_reward_2_list_all.append(fake_reward_2_list)\n","\n","    human_choice_list = torch.Tensor(human_choice_list)\n","    fake_reward_1_list_all = torch.Tensor(fake_reward_1_list_all)\n","    fake_reward_2_list_all = torch.Tensor(fake_reward_2_list_all)\n","\n","    # Update rewarders weights\n","    human_rewarder.update(\n","        fake_reward_1_list_all, fake_reward_2_list_all, human_choice_list\n","    )\n","  "]},{"cell_type":"markdown","metadata":{"id":"Fa2yXII_OpYz"},"source":["## Step 7: Pretraining HumanRewarder"]},{"cell_type":"markdown","metadata":{"id":"y6F8Vb8gOr_p"},"source":["In the Atari domain we also pretrain the reward predictor for 200 epochs before beginning RL training, to reduce the likelihood of irreversibly learning a bad policy based on an untrained predictor."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1676631941253,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"2nIaeNUvOp9X"},"outputs":[],"source":["def pretrainHumanRewarder(D_space):\n","    print(\"pretrainHumanRewarder: Start\")\n","    for epoch in range(0, 200):\n","        trainHumanRewarder(D_space)\n","\n","        if epoch % 20 == 0:\n","            print(\"pretrainHumanRewarder, Step:\", epoch, \"/200\")\n","\n","    print(\"pretrainHumanRewarder: End\")\n"]},{"cell_type":"markdown","metadata":{"id":"sD_23fpXu_pP"},"source":["## Step 8: Feeding D Space"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1676631941253,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"5gCHp3pIu_J0"},"outputs":[],"source":["# We shall call this function each T+5e6/5e6 timesteps\n","def feedingDSpace(\n","    obs1, obs2, D_space, real_human_check=False, real_reward_1=[], real_reward_2=[]\n","):\n","    if real_human_check:\n","        human_choice = getHumanChoice(obs1, obs2)\n","    else:\n","        # Here we fake the behavior of our real human assessor\n","        if np.array(real_reward_1).sum() > np.array(real_reward_2).sum():\n","            human_choice = [1, 0]\n","        else:\n","            human_choice = [0, 1]\n","\n","    if human_choice != [0, 0]:\n","        D_space.append(np.array([obs1, obs2, human_choice]))\n","\n","    # We keep only the 3000 lastest triplets\n","    if len(D_space) > 3000:\n","        D_space.pop(0)\n","\n","    return D_space"]},{"cell_type":"markdown","metadata":{"id":"MInbuPFGQ06r"},"source":["## Step 9: Train the Agent with A2C + HumanRewarder"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1676631941254,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"4ZOHpFc0UoZx"},"outputs":[],"source":["import imageio\n","from IPython.display import Image\n","from IPython.display import display\n","from ipywidgets import  widgets, HBox\n","\n","start_epoch = 0\n","scores = []\n","scores_window = deque(maxlen=20)\n","scores_fake = []\n","\n","scores_mean = []\n","scores_fake_mean = []"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1676631941254,"user":{"displayName":"Yanis L.","userId":"09738611231690158239"},"user_tz":-60},"id":"icTZE1R7Q0bG"},"outputs":[],"source":["def train_with_humanRewarder(n_episodes=1000, real_human_check=False):\n","    \"\"\"\n","    Params\n","    ======\n","        n_episodes (int): maximum number of training episodes\n","    \"\"\"\n","\n","    # We initialize our D space\n","    D_space = initialize_D_space(real_human_check)\n","\n","    # We pretrain our humanRewarder\n","    pretrainHumanRewarder(D_space)\n","\n","    T = 1  # Timestep over all the episodes\n","    timestep = 0  # timestep over one episoded\n","    labelled_number = 0  # Number of time we've asked the human to label 2 segments\n","\n","    scores = []\n","    scores_fake = []\n","    T_to_plot = []\n","\n","    obs_list = []\n","    real_reward_list = []\n","\n","    for i_episode in range(0, n_episodes + 1):\n","        obs, _ = env.reset()\n","        obs = np.array(obs._frames)\n","        obs = np.ascontiguousarray(obs, dtype=np.float32) / 255\n","\n","        score = 0\n","        score_fake = 0\n","        timestep = 0\n","\n","        while True:\n","            # Each 10 timestep we train our HumanRewarder\n","            if timestep % 10 == 0:\n","                trainHumanRewarder(D_space)\n","\n","            # We take an action and our agent learn from it\n","            action, log_prob, entropy = agent.act(obs)\n","            next_obs, reward, terminated, truncated, info = env.step(action)\n","            fake_reward = human_rewarder(obs)\n","            score_fake += torch.Tensor(fake_reward).mean().cpu().detach().numpy()\n","            score += reward\n","\n","            next_obs = np.array(next_obs._frames)\n","            next_obs = np.ascontiguousarray(next_obs, dtype=np.float32) / 255\n","\n","            # Each 5 timestep we train our Agent\n","            agent.step(\n","                obs,\n","                log_prob,\n","                entropy,\n","                fake_reward[0].mean().cpu().detach().numpy(),\n","                terminated or truncated,\n","                next_obs,\n","            )\n","\n","            obs = next_obs\n","\n","            ####################\n","            # Part feed D_space\n","\n","            obs_list.append(obs[-1])\n","            real_reward_list.append(reward)\n","            if len(obs_list) > 50:\n","                obs_list.pop(0)\n","                real_reward_list.pop(0)\n","\n","            # We do T*4 because each timestep is composed of 4 frames ? Not sure\n","            if (1 / 2) ** (((T * 1e3) + 5e6) / 5e6) > labelled_number:\n","                if len(obs_list) == 50:\n","                    print(\"Feed D_space\")\n","                    D_space = feedingDSpace(\n","                        obs_list[0:25],\n","                        obs_list[25:50],\n","                        D_space,\n","                        real_human_check,\n","                        real_reward_list[0:25],\n","                        real_reward_list[25:50],\n","                    )\n","                    obs_list = []\n","                    real_reward_list = []\n","                    labelled_number += 1\n","\n","            timestep += 1\n","            T += 1\n","\n","            if terminated or truncated:\n","                break\n","\n","        # if i_episode%10 == 0:\n","        #  for i in range(0, 3):\n","        #    torch.save(human_rewarder.nets[i].state_dict(), './Human_rewarder_saved_model_' + str(i))\n","        #  torch.save(agent.actor_net.state_dict(), './actor_net_saved_model')\n","        #  torch.save(agent.critic_net.state_dict(), './critic_net_saved_model')\n","\n","        scores.append(score)  # save most recent score\n","        scores_fake.append(score_fake)\n","        T_to_plot.append(T)\n","\n","        clear_output(True)\n","        plt.title(\"pong\")\n","        plt.plot(T_to_plot, scores, label=\"RL\")\n","        plt.plot(T_to_plot, scores_fake, label=\"3k synthetic labels\")\n","        plt.ylabel(\"reward\")\n","        plt.xlabel(\"timestep\")\n","\n","        plt.legend(loc=4)\n","\n","        plt.show()\n","        print(\"Episode:\", i_episode)\n","\n","    return scores"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z79ruyVlVfKJ","outputId":"fc5d4f70-b480-452d-aa1f-8fcc8593560d"},"outputs":[{"name":"stdout","output_type":"stream","text":["initialize_D_space: Start\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-16-457364a0b847>:48: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  D_space.append(np.array([obs_1_list, obs_2_list, human_choice]))\n"]}],"source":["scores = train_with_humanRewarder(10000, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LlfArHXWT5wx"},"outputs":[],"source":["#torch.save(human_rewarder.net.state_dict(), './Human_rewarder_saved_model')\n","#human_rewarder.net.load_state_dict(torch.load('Human_rewarder_saved_model'))\n"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","provenance":[{"file_id":"16Igp436f-k8Pnc77d9gC31GxOl9r0vvR","timestamp":1675196606226}],"toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
